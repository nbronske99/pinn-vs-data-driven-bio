{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==================== LOAD SYNTHETIC DATA ====================\n",
        "data = np.load('data/synthetic_m.npz')\n",
        "t = data['t']\n",
        "V = data['V']\n",
        "m_true = data['m_true']\n",
        "\n",
        "print(\"Data loaded!\")\n",
        "print(f\"   Time points: {len(t)}\")\n",
        "print(f\"   Training on (t, V) â†’ m(t)\")\n",
        "\n",
        "# Prepare tensors\n",
        "X = torch.tensor(np.stack([t, V], axis=1), dtype=torch.float32)  # shape: (1000, 2)\n",
        "y = torch.tensor(m_true, dtype=torch.float32).unsqueeze(1)        # shape: (1000, 1)\n",
        "\n",
        "# ==================== SIMPLE DATA-DRIVEN NN ====================\n",
        "class DataDrivenNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = DataDrivenNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ==================== TRAINING LOOP ====================\n",
        "epochs = 2000\n",
        "losses = []\n",
        "\n",
        "print(\"Training data-driven baseline (no physics)...\")\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(X)\n",
        "    loss = criterion(pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    if epoch % 400 == 0:\n",
        "        print(f\"   Epoch {epoch:4d} | Loss: {loss.item():.6f}\")\n",
        "\n",
        "# ==================== PLOTS ====================\n",
        "m_pred = model(X).detach().numpy().flatten()\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "axs[0].plot(t, V, 'b-', label='Voltage protocol')\n",
        "axs[0].set_ylabel('Voltage (mV)')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(t, m_true, 'r-', label='True m(t)', linewidth=2)\n",
        "axs[1].plot(t, m_pred, 'g--', label='Data-driven NN prediction', linewidth=2)\n",
        "axs[1].set_xlabel('Time (ms)')\n",
        "axs[1].set_ylabel('Gating variable m(t)')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.suptitle('Data-Driven Neural Network Baseline (no physics prior)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/02_data_driven_baseline.png', dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Training complete! Plot saved to outputs/02_data_driven_baseline.png\")\n",
        "print(f\"   Final MSE: {losses[-1]:.6f}\")"
      ],
      "metadata": {
        "id": "CYiz7HK0ZQQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}