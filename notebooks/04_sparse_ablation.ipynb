{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 — Sparse-Data Ablation Study\n",
        "\n",
        "**Core experiment:** How do PINNs and data-driven NNs compare as training data becomes scarce?\n",
        "\n",
        "We train both models on subsets of 25, 50, 100, 200, 500, and 800 points,  \n",
        "evaluate on a fixed 200-point test set, and plot generalization error vs. training set size.\n",
        "\n",
        "Hypothesis: the PINN's physics constraint acts as a strong regularizer,  \n",
        "so it should degrade much more gracefully than the data-driven model under sparse data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import odeint\n",
        "\n",
        "# ==================== LOAD DATA ====================\n",
        "data = np.load('data/synthetic_m.npz')\n",
        "t = data['t']\n",
        "V = data['V']\n",
        "m_true = data['m_true']\n",
        "\n",
        "X_all = np.stack([t, V], axis=1)\n",
        "\n",
        "# Fixed test set: 200 randomly chosen points, held constant across all experiments\n",
        "np.random.seed(42)\n",
        "all_idx = np.random.permutation(len(t))\n",
        "test_idx = all_idx[:200]\n",
        "pool_idx = all_idx[200:]  # remaining 800 points to subsample from\n",
        "\n",
        "X_test = torch.tensor(X_all[test_idx], dtype=torch.float32)\n",
        "y_test = torch.tensor(m_true[test_idx], dtype=torch.float32).unsqueeze(1)\n",
        "X_full = torch.tensor(X_all, dtype=torch.float32)\n",
        "\n",
        "print(f'Fixed test set: {len(test_idx)} points')\n",
        "print(f'Training pool:  {len(pool_idx)} points')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== HH RATE FUNCTIONS (PyTorch) ====================\n",
        "def alpha_m(V):\n",
        "    return 0.1 * (V + 40.0) / (1.0 - torch.exp(-(V + 40.0) / 10.0))\n",
        "\n",
        "def beta_m(V):\n",
        "    return 4.0 * torch.exp(-(V + 65.0) / 18.0)\n",
        "\n",
        "# ==================== MODEL DEFINITIONS ====================\n",
        "class DataDrivenNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 64), nn.Tanh(),\n",
        "            nn.Linear(64, 64), nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def physics_residual(self, x):\n",
        "        x = x.clone().requires_grad_(True)\n",
        "        m_pred = self(x)\n",
        "        dm_dt = torch.autograd.grad(\n",
        "            m_pred, x, grad_outputs=torch.ones_like(m_pred),\n",
        "            create_graph=True\n",
        "        )[0][:, 0:1]\n",
        "        V = x[:, 1:2]\n",
        "        return dm_dt - (alpha_m(V) * (1 - m_pred) - beta_m(V) * m_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== TRAINING FUNCTIONS ====================\n",
        "def train_data_driven(X_train, y_train, epochs=2000, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    model = DataDrivenNN()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X_train), y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_mse = criterion(model(X_test), y_test).item()\n",
        "    return test_mse, model\n",
        "\n",
        "\n",
        "def train_pinn(X_train, y_train, X_colloc, epochs=3000, lambda_phys=0.1, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    model = PINN()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        data_loss = criterion(model(X_train), y_train)\n",
        "        phys_res = model.physics_residual(X_colloc)\n",
        "        phys_loss = torch.mean(phys_res ** 2)\n",
        "        loss = data_loss + lambda_phys * phys_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_mse = criterion(model(X_test), y_test).item()\n",
        "    return test_mse, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== ABLATION SWEEP ====================\n",
        "train_sizes = [25, 50, 100, 200, 500, 800]\n",
        "n_trials = 3  # average over random subsets for robustness\n",
        "\n",
        "dd_results = {n: [] for n in train_sizes}\n",
        "pinn_results = {n: [] for n in train_sizes}\n",
        "\n",
        "for n_train in train_sizes:\n",
        "    print(f'\\n===== Training size: {n_train} =====')\n",
        "    for trial in range(n_trials):\n",
        "        # Subsample from pool\n",
        "        rng = np.random.RandomState(trial * 100 + n_train)\n",
        "        sub_idx = rng.choice(pool_idx, size=n_train, replace=False)\n",
        "\n",
        "        X_train = torch.tensor(X_all[sub_idx], dtype=torch.float32)\n",
        "        y_train = torch.tensor(m_true[sub_idx], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Data-driven\n",
        "        dd_mse, _ = train_data_driven(X_train, y_train, seed=trial)\n",
        "        dd_results[n_train].append(dd_mse)\n",
        "\n",
        "        # PINN (collocation on full domain — physics doesn't need labels)\n",
        "        pinn_mse, _ = train_pinn(X_train, y_train, X_full, seed=trial)\n",
        "        pinn_results[n_train].append(pinn_mse)\n",
        "\n",
        "        print(f'   Trial {trial+1}: DD={dd_mse:.6f}  PINN={pinn_mse:.6f}')\n",
        "\n",
        "print('\\nAblation sweep complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== RESULTS TABLE ====================\n",
        "dd_means  = [np.mean(dd_results[n]) for n in train_sizes]\n",
        "dd_stds   = [np.std(dd_results[n]) for n in train_sizes]\n",
        "pinn_means = [np.mean(pinn_results[n]) for n in train_sizes]\n",
        "pinn_stds  = [np.std(pinn_results[n]) for n in train_sizes]\n",
        "\n",
        "print(f'{\"N_train\":>8s}  {\"DD MSE\":>12s}  {\"PINN MSE\":>12s}  {\"Improvement\":>12s}')\n",
        "print('-' * 52)\n",
        "for i, n in enumerate(train_sizes):\n",
        "    improv = (dd_means[i] - pinn_means[i]) / dd_means[i] * 100\n",
        "    print(f'{n:>8d}  {dd_means[i]:>10.6f}±{dd_stds[i]:.4f}  {pinn_means[i]:>10.6f}±{pinn_stds[i]:.4f}  {improv:>+10.1f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== MAIN PLOT: MSE vs TRAINING SIZE ====================\n",
        "fig, ax = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "ax.errorbar(train_sizes, dd_means, yerr=dd_stds,\n",
        "            marker='o', capsize=5, linewidth=2, markersize=8,\n",
        "            label='Data-Driven NN (ReLU)', color='#e74c3c')\n",
        "ax.errorbar(train_sizes, pinn_means, yerr=pinn_stds,\n",
        "            marker='s', capsize=5, linewidth=2, markersize=8,\n",
        "            label='PINN (Tanh + ODE constraint)', color='#2ecc71')\n",
        "\n",
        "ax.set_xlabel('Number of Training Points', fontsize=13)\n",
        "ax.set_ylabel('Test MSE', fontsize=13)\n",
        "ax.set_title('Generalization Error vs. Training Set Size\\n(Hodgkin-Huxley m-gate)', fontsize=14)\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "ax.grid(True, which='both', alpha=0.3)\n",
        "ax.set_xticks(train_sizes)\n",
        "ax.set_xticklabels(train_sizes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/04_sparse_ablation.png', dpi=200)\n",
        "plt.show()\n",
        "print('Plot saved to outputs/04_sparse_ablation.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== BONUS: PREDICTION CURVES AT KEY SIZES ====================\n",
        "highlight_sizes = [25, 100, 800]\n",
        "fig, axs = plt.subplots(len(highlight_sizes), 2, figsize=(14, 4 * len(highlight_sizes)))\n",
        "\n",
        "for row, n_train in enumerate(highlight_sizes):\n",
        "    rng = np.random.RandomState(n_train)  # reproducible subset\n",
        "    sub_idx = rng.choice(pool_idx, size=n_train, replace=False)\n",
        "    X_tr = torch.tensor(X_all[sub_idx], dtype=torch.float32)\n",
        "    y_tr = torch.tensor(m_true[sub_idx], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    _, dd_model = train_data_driven(X_tr, y_tr, seed=99)\n",
        "    _, pinn_model = train_pinn(X_tr, y_tr, X_full, seed=99)\n",
        "\n",
        "    dd_pred   = dd_model(X_full).detach().numpy().flatten()\n",
        "    pinn_pred = pinn_model(X_full).detach().numpy().flatten()\n",
        "\n",
        "    for col, (pred, name) in enumerate([(dd_pred, 'Data-Driven'), (pinn_pred, 'PINN')]):\n",
        "        axs[row, col].plot(t, m_true, 'r-', label='True m(t)', lw=2)\n",
        "        axs[row, col].plot(t, pred, 'g--', label=f'{name} pred', lw=2)\n",
        "        axs[row, col].scatter(t[sub_idx], m_true[sub_idx], c='blue', s=10, zorder=5, alpha=0.5, label='Train pts')\n",
        "        axs[row, col].set_title(f'{name} — {n_train} training points')\n",
        "        axs[row, col].legend(fontsize=8); axs[row, col].grid(True)\n",
        "        axs[row, col].set_ylim(-0.1, 1.1)\n",
        "\n",
        "plt.suptitle('Prediction Quality at Different Training Sizes', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/04_sparse_ablation_curves.png', dpi=200)\n",
        "plt.show()\n",
        "print('Curve comparison saved to outputs/04_sparse_ablation_curves.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== SAVE ABLATION RESULTS ====================\n",
        "np.savez('outputs/04_ablation_results.npz',\n",
        "         train_sizes=np.array(train_sizes),\n",
        "         dd_means=np.array(dd_means),\n",
        "         dd_stds=np.array(dd_stds),\n",
        "         pinn_means=np.array(pinn_means),\n",
        "         pinn_stds=np.array(pinn_stds))\n",
        "print('Ablation results saved.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
